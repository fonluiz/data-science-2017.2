---
title: "Hands On Caret "
subtitle: "A library for creating predictive models"
author: "Luiz Fonseca"
date: "12/07/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Caret is the short for *C*lassification *A*nd *RE*gression *T*raining. It is a complete package that covers all the stages of a pipeline for creating a machine learning predictive model. In this tutorial, I will explain the following topics:

1. How to install caret
2. How to create a simple model
3. How to use cross-validation to avoid overfitting
4. How to add simple preprocessing to your data
5. How to find the best parameters for your choosen model
6. How to see the most important features/variables for your model
7. How to use your model to predict

# Installing

Installing caret is just as installing any other package in R. Just use the code below. If you're using RStudio (which is recommended), you can also install it by clicking in "tools" > "Install Packages..." in the toolbar.

```{r, eval=FALSE}
install.packages("caret")
```

# Creating a simple model

We're gonna do that by using the **train()** function. The function **train()** is a core function of caret. As its name suggests, it is used to train a model, that is, to apply an algorithm to a set of data and create a model which represents that dataset.

The train() function has three basic parameters:

1. Formula
2. Dataset
3. Method (or algorithm)

The **formula** parameter is where you specify what is your dependent variable (what you want to predict) and independent variables (features). I'll explain more about how to write your formula below. 

The **dataset** parameter is your data. 

The **method** parameter is a string specifying which classification or regression model to use.

In this tutorial I'm using the mtcars dataset. It is one of the built-in R datasets. Below, there is an explanation about this dataset: 

### Motor Trend Car Road Tests (mtcars)
#### Description
The data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973â€“74 models).

Format
A data frame with 32 observations on 11 variables.

1.	mpg	Miles/(US) gallon
2.	cyl	Number of cylinders
3.	disp	Displacement (cu.in.)
4.	hp	Gross horsepower
5.	drat	Rear axle ratio
6.	wt	Weight (1000 lbs)
7.	qsec	1/4 mile time
8.	vs	V/S
9.	am	Transmission (0 = automatic, 1 = manual)
10.	gear	Number of forward gears
11.	carb	Number of carburetors

let's take a look on the dataframe.

```{r}
data(mtcars)    # Load the dataset
head(mtcars)    
?mtcars         # Get more information about this dataset 
```

Now, let's create regression models to predict how many miles per gallon (mpg) a car model can reach based on the other attributes.

The **formula** can be written as **x ~ y, z, w** where x is the dependent variable, mpg in our case, and y, z and w are independent variables. If you want to pass all the other attributes you can write it as **x ~ .**.

```{r}
library(caret)

# Simple linear regression model (lm means linear model)
model <- train(mpg ~ wt,
               data = mtcars,
               method = "lm")

# Multiple linear regression model
model <- train(mpg ~ .,
               data = mtcars,
               method = "lm")

# Ridge regression model
model <- train(mpg ~ .,
               data = mtcars,
               method = "ridge") # Try using "lasso"

```

That's how you can use the function **train()** to create different basic models. Easy, isn't it?

# K-fold cross-validation

The function train() has other optional parameters. Let's learn how to add resampling to our model by adding the parameter **trControl** (train control) to our train function.

The resampling process can be done by using K-fold cross-validation, leave-one-out cross-validation or bootstrapping. We are going to use 10-fold cross-validation in this example. To achieve that, we need to use another Caret function, **trainControl()**. Check the code, below.

```{r}
## 10-fold CV
fitControl <- trainControl(method = "repeatedcv",   # other possible values: boot", "boot632", "cv", "repeatedcv", "LOOCV", "LGOCV"
                           number = 10,     # number of folds
                           repeats = 10)    # repeated ten times

model.cv <- train(mpg ~ .,
               data = mtcars,
               method = "lasso",            # now we're using the lasso method
               trControl = fitControl)  

model.cv
```

# Adding preprocessing

The **train()** function has another optional parameter called **preProcess**. It's used to add some pre-processing to your data.

In this example we're going to use the following pre-processing:

1. center data (i.e. compute the mean for each column and subtracts it from each respective value);
2. scale data (i.e. put all data in the same scale, e.g. from  0 up to 1)

However, there are more pre-processing possibilities such as "BoxCox", "YeoJohnson", "expoTrans", "range", "knnImpute", "bagImpute", "medianImpute", "pca", "ica" and "spatialSign".

```{r}
model.cv <- train(mpg ~ .,
               data = mtcars,
               method = "lasso",
               trControl = fitControl,
               preProcess = c('scale', 'center')) # default: no pre-processing

?train
model.cv
```

# Finding the model hyper-parameters

We can find the best hyper-parameters for our model by using the **tuneGrid** parameter. This parameter receives A data frame with possible tuning values. The columns are named the same as the tuning parameters. 

To generate the possible values, I am going to use the **expand.grid** from the base library. To explain the use of **tuneGrid** I'm gonna use the ridge regression method. 

##### Short explanation
The ridge method shrinks the coefficients of the predictor variables towards 0, as lambda grows. That shrinking effect decreases the model flexibility, decreasing its variance as well, but increasing bias. The ideia of Ridge  regression is to find a value for lambda that is a satisfying trade-off  between bias and variance.

With the code below we can find the best lambda parameter for ridge regression between 10^-2 up to 10^10.
```{r}
# Here I generate a dataframe with a column named lambda with 100 values that goes from 10^10 to 10^-2
lambdaGrid <- expand.grid(lambda = 10^seq(10, -2, length=100))

model.cv <- train(mpg ~ .,
               data = mtcars,
               method = "ridge",
               trControl = fitControl,
               preProcess = c('scale', 'center'),
               tuneGrid = lambdaGrid,                  # Test all the lambda values in the lambdaGrid dataframe
               na.action = na.omit)                    # Ignore NA values

model.cv
```

When you call **model.cv**, you can see the metrics RMSE, Rsquared and MAE for each lambda value that you tested and the model also outputs the best choice for lambda among the values tested. In this case, it was lambda =  0.1232847.

There's another way of searching for hyper-parameter without passing a list of values to the **train()** function. We can use **search = "random**  inside **trainControl()** and the function will automatically test a range of values.

```{r}
fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           repeats = 10,
                           search = "random")  # hyper-parameters random search 

model.cv <- train(mpg ~ .,
               data = mtcars,
               method = "ridge",
               trControl = fitControl,
               preProcess = c('scale', 'center'),
               na.action = na.omit)

model.cv
```

# Variable Importance

Now let's learn how to see which are the most important variables for our model. We can use the Caret function **varImp**. The return of **varImp** can be passed to the function **ggplot** to generate a visualization.

```{r}
ggplot(varImp(model.cv))
```

As we can see in the graphic, the displacement variable is them ost important for our predictive model.

# Predictions

At last, we can use the function **predict** to predict a car's performance, that is, how many miles it can reach per gallon. I'm gonna pass as argument the same dataframe used to generate the model just  to show how the function works.

In a real project you would use a bigger dataframe and separate it into a train set and a test set, but that's not the purpose here.

```{r}
predictions <- predict(model.cv, mtcars)

predictions
```

You can compare the values predicted with the real values. There are some metrics that you can use to see if you're model is a good predictor, but we are not going to get into that here.

Thank you for reading. I hope it is useful.